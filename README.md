# ğŸµ Sparkify Data Pipeline with Apache Airflow
![Airflow](https://upload.wikimedia.org/wikipedia/commons/d/de/AirflowLogo.png)

> **Automating an ETL pipeline with Apache Airflow, S3, and Redshift for music streaming data processing.**

---

## ğŸ“œ Introduction
Sparkify, a music streaming company, aims to **automate its data pipeline** for better user data analysis.  
This project uses **Apache Airflow** to **orchestrate an ETL pipeline**, **Amazon S3** as the data source, and **Amazon Redshift** as the data warehouse.

---

## ğŸ“Œ Project Architecture

```mermaid
graph TD
    subgraph S3 [Amazon S3]
        A[Log Data JSON] -->|S3 Bucket| B[Stage Events Table]
        A2[Song Data JSON] -->|S3 Bucket| B2[Stage Songs Table]
    end

    subgraph Redshift [Amazon Redshift]
        B --> C[Fact Table: Songplays]
        B2 --> C
        C --> D[Dim: Users]
        C --> E[Dim: Songs]
        C --> F[Dim: Artists]
        C --> G[Dim: Time]
    end

    subgraph Airflow [Apache Airflow DAG]
        Start -->|Stage Data| B
        Start -->|Stage Data| B2
        B & B2 -->|Load Fact| C
        C -->|Load Dimensions| D & E & F & G
        D & E & F & G -->|Data Quality Check| QualityCheck
        QualityCheck --> End
    end
```

# ğŸ“‚ Directory & Files
```ğŸ› ï¸ Project Structure

.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ final_project.py  # Main DAG definition
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”œâ”€â”€ stage_redshift.py    # Staging data from S3 to Redshift
â”‚   â”‚   â”œâ”€â”€ load_fact.py         # Loading fact table
â”‚   â”‚   â”œâ”€â”€ load_dimension.py    # Loading dimension tables
â”‚   â”‚   â”œâ”€â”€ data_quality.py      # Data quality checks
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â”œâ”€â”€ sql_queries.py       # SQL queries for data transformation
â”œâ”€â”€ create_tables.sql            # SQL script for Redshift table creation
â”œâ”€â”€ docker-compose.yaml          # Local Airflow environment setup
â””â”€â”€ README.md
```

# âš¡ Apache Airflow DAG

Graph View
Task Dependencies

```mermaid
graph TD;
    A[Begin Execution] --> B[Stage Events]
    A --> C[Stage Songs]
    B --> D[Load Songplays Fact Table]
    C --> D
    D --> E[Load User Dimension]
    D --> F[Load Song Dimension]
    D --> G[Load Artist Dimension]
    D --> H[Load Time Dimension]
    E --> I[Run Data Quality Checks]
    F --> I
    G --> I
    H --> I
    I --> J[End Execution]
```

# ğŸ› ï¸ Installation

ğŸ”¹ Requirements

    Python 3.7+
    Docker (for local Airflow execution)
    AWS CLI (to connect to S3 and Redshift)

ğŸ”¹ Setup Environment
ğŸ”¹ Start Docker with Airflow

docker-compose up -d

ğŸ”¹ Set AWS Variables in Airflow

airflow connections add aws_credentials \
    --conn-type aws \
    --conn-extra '{"aws_access_key_id": "YOUR_ACCESS_KEY", "aws_secret_access_key": "YOUR_SECRET_KEY"}'

# ğŸ“‚ Module Breakdown


1ï¸âƒ£ StageToRedshiftOperator

ğŸ“Œ Loads JSON data from S3 into Redshift using COPY

copy_sql = f"""
    COPY {self.table}
    FROM '{s3_path}'
    IAM_ROLE '{self.iam_role}'
    FORMAT AS JSON '{self.json_format}';
"""

2ï¸âƒ£ LoadFactOperator

ğŸ“Œ Loads data into the fact table

INSERT INTO songplays (playid, start_time, userid, ...)
SELECT DISTINCT md5(events.sessionid || events.start_time), ...
FROM staging_events events
LEFT JOIN staging_songs songs ON events.song = songs.title;

3ï¸âƒ£ LoadDimensionOperator

ğŸ“Œ Supports truncate-insert

if self.truncate_insert:
    redshift.run(f"DELETE FROM {self.table}")
redshift.run(self.sql_query)

4ï¸âƒ£ DataQualityOperator

ğŸ“Œ Performs data integrity checks in Redshift

for check in self.sql_checks:
    sql_query, expected_value = check
    records = redshift.get_records(sql_query)
    if records[0][0] < expected_value:
        raise ValueError(f"Test failed: {sql_query}")


# ğŸ”¬ Testing & Debugging

âœ… Test Airflow DAG

airflow tasks test final_project Stage_events 2024-02-01

âœ… Debug Logs

docker logs -f airflow_scheduler

âœ… Verify Redshift

SELECT COUNT(*) FROM songplays;

# ğŸ–¥ï¸ Expected Output

[INFO] StageToRedshiftOperator - Copying data from S3 to Redshift...
[INFO] LoadFactOperator - Inserting data into fact table...
[INFO] LoadDimensionOperator - Inserting data into dimension tables...
[INFO] DataQualityOperator - Running data quality checks...
[INFO] DAG Execution Completed Successfully!

# ğŸ“ˆ Project Benefits

âœ… Automation â†’ The entire ETL process is managed by Airflow
âœ… Scalability â†’ AWS Redshift handles large-scale data
âœ… Reliability â†’ Data quality tests ensure integrity
âœ… Modularity â†’ Easily extendable with new sources and transformations
ğŸ“Œ Conclusion

This project demonstrates how Apache Airflow can manage a complex data pipeline with AWS S3 and Redshift.
The workflow automates data ingestion, transformation, and validation, enabling efficient analytics.

# ğŸš€ Next Steps:

    Integrate Amazon Athena for ad-hoc analysis
    Optimize SQL queries for better performance
    Build a dashboard with Amazon QuickSight to visualize data insights

# ğŸ”— Useful Links

    Apache Airflow Documentation
    Amazon Redshift Best Practices
    S3 Data Management

# ğŸ¯ Built with â¤ï¸ for Sparkify ğŸµ
Author: Riccardo Scaringi
ğŸ“… Last Updated: February 2025
